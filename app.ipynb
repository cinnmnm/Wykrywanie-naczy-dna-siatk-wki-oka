{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9185ae46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DLUnet v2.0.0] Retinal Vessel Segmentation Package Loaded\n",
      "Available interfaces:\n",
      "  • VesselSegmentationPipeline - Complete training/inference pipeline\n",
      "  • VesselSegmentationController - Simple controller interface\n",
      "  • Core components for custom implementations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f4f8868c33a4b0f83f17f842b2c5e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(FileUpload(value=(), accept='image/*', description='Choose Image', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from App.GUI import GUI\n",
    "from App.DemoController import DemoController\n",
    "import os\n",
    "\n",
    "print(\"🩸👁️ Retinal Vessel Segmentation Application\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Initialize demo controller (automatically loads demo model)\n",
    "controller = DemoController(auto_load_demo=True)\n",
    "\n",
    "print(f\"\\n📋 Available segmentation methods: {controller.get_available_methods()}\")\n",
    "\n",
    "if 'deep_learning' in controller.get_available_methods():\n",
    "    model_info = controller.get_model_info()\n",
    "    print(f\"\\n🤖 Deep Learning Model Info:\")\n",
    "    print(f\"   Model Type: {model_info['model_type']}\")\n",
    "    print(f\"   Parameters: {model_info['parameters']:,}\")\n",
    "    print(f\"   Device: {model_info['device']}\")\n",
    "    print(f\"   Target Size: {model_info['target_size']}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Deep learning model not available, using filter-based segmentation only\")\n",
    "\n",
    "print(\"\\n🖥️ Initializing GUI...\")\n",
    "\n",
    "# Initialize and launch GUI\n",
    "gui = GUI(controller)\n",
    "gui.init()\n",
    "\n",
    "print(\"✅ GUI ready! Use the interface above to:\")\n",
    "print(\"   1. Upload retinal images\")\n",
    "print(\"   2. Select segmentation method(s):\")\n",
    "print(\"      • Prosty Model = Filter-based segmentation\")\n",
    "print(\"      • DL Model = Deep learning segmentation\") \n",
    "print(\"   3. Click Start to process\")\n",
    "print(\"\\n🎯 Ready for vessel segmentation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadd0e88",
   "metadata": {},
   "source": [
    "# Wykrywanie naczyń dna siatkówki oka\n",
    "\n",
    "## Opis\n",
    "\n",
    "Przygotowanie aplikacji, która dla zadanego obrazu wejściowego przedstawiającego dno siatkówki oka automatycznie wykrywa naczynia krwionośne. Z formalnego punktu widzenia dla każdego piksela wykorzystany algorytm musi stwierdzić, czy ten piksel stanowi naczynie krwionośne, czy nie (klasyfikacja binarna).\n",
    "\n",
    "## Wymagania obowiązkowe\n",
    "\n",
    "Algorytm w podstawowej wersji powinien wykorzystywać techniki przetwarzania obrazu do detekcji naczyń krwionośnych. W ramach takiego procesu przetwarzania można wyróżnić 3 główne elementy:\n",
    "\n",
    "1. **Wstępne przetworzenie obrazu**  \n",
    "   Wejściowy obraz może być zaszumiony/zbyt ciemny/jasny. Można tutaj wykorzystać takie techniki jak: rozmycie, wyostrzenie, normalizacja histogramu kolorów itp.\n",
    "\n",
    "2. **Właściwe przetworzenie obrazu**  \n",
    "   W celu wyodrębnienia naczyń krwionośnych można zastosować różne techniki wykrywania krawędzi (np. filtr Frangi’ego).\n",
    "\n",
    "3. **Końcowe przetwarzanie obrazu**  \n",
    "   Przetwarzanie uzyskanego obrazu w celu poprawy skuteczności wykrywania naczyń poprzez “naprawę” błędów z poprzednich kroków.\n",
    "\n",
    "Wynik należy wizualizować, np. zamalowując wyróżniającym się kolorem piksele zaklasyfikowane jako naczynie krwionośne. Najlepiej wygenerować binarną maskę odpowiedzi algorytmu, która zostanie potem wykorzystana do analizy statystycznej (porównanie z maską ekspercką z ręcznie zaznaczonymi naczyniami).\n",
    "\n",
    "Ważnym elementem oceny jest skuteczność algorytmu. W tym celu należy dokonać podstawowej analizy statystycznej jakości działania algorytmu. Działanie programu należy przetestować na minimum 5 obrazach. Podczas testów należy wyznaczyć:\n",
    "\n",
    "- macierze pomyłek,\n",
    "- trafność (accuracy),\n",
    "- czułość (sensitivity),\n",
    "- swoistość (specificity).\n",
    "\n",
    "Przy wyznaczaniu czułości i swoistości należy założyć, że naczynie to klasa pozytywna, a tło — negatywna. Ponieważ mamy do czynienia z niezrównoważonym rozkładem klas, należy dodatkowo wykorzystać miary dla danych niezrównoważonych (np. średnią arytmetyczną lub geometryczną czułości i swoistości).\n",
    "\n",
    "## Wymagania na 4.0\n",
    "\n",
    "Po wstępnym przetworzeniu obrazu należy podzielić go na wycinki (np. 5x5 px) i dla każdej z nich dokonać ekstrakcji cech z obrazu: np. wariancja kolorów, momenty centralne, momenty Hu itp. Wartości te wraz z informacją pochodzącą z maski eksperckiej (decyzja dla środkowego piksela wycinka) stanowić będą zbiór danych wykorzystany do budowy wybranego klasyfikatora, prostszego niż głęboka sieć neuronowa (np. kNN, drzewo lub las decyzyjny, SVM). Należy skorzystać z gotowej implementacji klasyfikatora (np. w bibliotece scikit-learn).\n",
    "\n",
    "Z uwagi na ograniczenia pamięciowe konieczne może być ograniczenie rozmiaru zbioru uczącego poprzez losowy wybór punktów (możliwość zastosowania undersamplingu do zrównoważenia rozkładu klas w zbiorze uczącym).\n",
    "\n",
    "Zdolności predykcyjne tak opracowanego klasyfikatora należy wstępnie zweryfikować na niezależnym zbiorze testowym hold-out (np. pochodzącym z innej części obrazu lub z innego obrazu).\n",
    "\n",
    "Gotowy klasyfikator powinien zostać osadzony w aplikacji, a jego działanie powinno zostać zwizualizowane i przetestowane w taki sam sposób, jak działanie technik przetwarzania obrazu z wymagań podstawowych.\n",
    "\n",
    "## Wymagania na 5.0\n",
    "\n",
    "Jako model decyzyjny należy wykorzystać głęboką sieć neuronową. W zależności od wybranego rodzaju sieci, może zostać ona nauczona na wycinkach obrazu (podobnie jak w przypadku wymagań na 4.0), jak i na całych obrazach (np. w przypadku sieci UNet). Należy skorzystać z gotowej implementacji sieci (np. w bibliotece Keras, PyTorch lub TensorFlow).\n",
    "\n",
    "Zdolności predykcyjne nauczonej sieci neuronowej powinny być wstępnie zweryfikowane na zbiorze testowym hold-out.\n",
    "\n",
    "Nauczona sieć powinna zostać osadzona w aplikacji i tam dodatkowo przetestowana zgodnie z wymaganiami obowiązkowymi.\n",
    "\n",
    "## Uwaga\n",
    "\n",
    "W projekcie należy skorzystać z jednej z dostępnych baz danych z obrazami (patrz linki poniżej) — ta sama baza powinna być stosowana we wszystkich krokach projektu.\n",
    "\n",
    "## Linki\n",
    "\n",
    "- Baza obrazów HRF: https://www5.cs.fau.de/research/data/fundus-images/  \n",
    "  (proszę się nie przejmować komunikatem “website deprecated and outdated”).\n",
    "\n",
    "- Baza obrazów STARE: http://cecas.clemson.edu/~ahoover/stare/probing/  \n",
    "- Baza obrazów CHASE: https://blogs.kingston.ac.uk/retinal/chasedb1/  \n",
    "  (przeglądanie zdjęć nie działa, można jednak pobrać całe archiwum)\n",
    "\n",
    "- Biblioteka scikit-learn: https://scikit-learn.org/stable/  \n",
    "- Biblioteka imbalanced-learn: https://imbalanced-learn.org/stable/  \n",
    "\n",
    "- P. Liskowski, K. Krawiec: *Segmenting Retinal Blood Vessels With Deep Neural Networks*: https://ieeexplore.ieee.org/document/7440871  \n",
    "\n",
    "- Implementacja sieci UNet z wykorzystaniem biblioteki PyTorch:  \n",
    "  - https://pyimagesearch.com/2023/11/06/image-segmentation-with-u-net-in-pytorch-the-grand-finale-of-the-autoencoder-series/  \n",
    "  - https://pyimagesearch.com/2021/11/08/u-net-training-image-segmentation-models-in-pytorch/  \n",
    "\n",
    "- MONAI: Medical Open Network for AI: https://monai.io/  \n",
    "\n",
    "- Przykładowa implementacja sieci UNet z wykorzystaniem biblioteki Keras:  \n",
    "  - https://github.com/zhixuhao/unet  \n",
    "  - https://github.com/karolzak/keras-unet  \n",
    "  - https://github.com/qubvel/segmentation_models  \n",
    "\n",
    "- Segmentacja obrazu z wykorzystaniem TensorFlow: https://www.tensorflow.org/tutorials/images/segmentation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
